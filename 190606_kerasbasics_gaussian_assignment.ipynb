{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190606_kerasbasics_gaussian-assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonudk/Source-Code-from-Tutorials/blob/master/190606_kerasbasics_gaussian_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwu_H0V1M9A6",
        "colab_type": "text"
      },
      "source": [
        "# Keras Basics\n",
        "We will learn about\n",
        "* Dense layers\n",
        "* Categorical cross-entropy\n",
        "\n",
        "A toy example to show how to train a classifier with Keras and use it. The data comes from three gaussian distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_F5JP-dM9A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DATA GENERATION\n",
        "import numpy as np\n",
        "\n",
        "def generateX(cls):\n",
        "    '''\n",
        "    Inputs:\n",
        "        cls: class {0, 1, 2}\n",
        "    Outputs:\n",
        "        a sample from cls\n",
        "    '''\n",
        "    assert cls in [0,1,2]\n",
        "    if cls==0:\n",
        "        return np.random.normal(np.array([0,0]),100)# what this means\n",
        "    elif cls==1:\n",
        "        return np.random.normal(np.array([200,200]),100)\n",
        "    elif cls==2:\n",
        "        return np.random.normal(np.array([-200,200]),100)\n",
        "    assert False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8C0ZHLrM9BM",
        "colab_type": "text"
      },
      "source": [
        "Could you write a function to generate N samples from class 0 and N samples from class 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS3llbqhM9BO",
        "colab_type": "code",
        "outputId": "b51db785-13d9-4a0b-d30e-ba3b65c2eabf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2635
        }
      },
      "source": [
        "def generateXY(N):\n",
        "    '''\n",
        "    Inputs:\n",
        "        N: no. of samples of each class\n",
        "    '''\n",
        "    Y = []\n",
        "    X = []\n",
        "    for cls in range(3):\n",
        "        for i in range(N):\n",
        "            y = cls\n",
        "            x = generateX(y)\n",
        "            x = x.reshape(-1, 1).T\n",
        "            Y.append(y)\n",
        "            X.append(x)\n",
        "    X = np.concatenate(X, axis=0)\n",
        "    print(X.shape, 'X.shape')\n",
        "    print(np.array(Y).shape)\n",
        "    Y = np.array(Y).reshape(-1,1)\n",
        "    print(X.shape)\n",
        "    print(Y.shape)\n",
        "    return X, Y\n",
        "\n",
        "# def generateXY(N):\n",
        "#     '''\n",
        "#     Inputs:\n",
        "#         N: no. of samples of each class\n",
        "#     '''\n",
        "#     Y = [0]*N + [1]*N + [2]*N   # a list of 0s and 1s as ground truth classes  \n",
        "#     X = [generateX(y) for y in Y]  # samples corresponding to the ground truth Y\n",
        "#     X = np.vstack(X)   # arrange samples in different rows\n",
        "#     Y = np.array(Y).reshape(-1,1)\n",
        "#     return X, Y\n",
        "\n",
        "X_train, Y_train_int = generateXY(50)\n",
        "# Nx = X_train.shape\n",
        "print(Y_train_int)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 2) X.shape\n",
            "(150,)\n",
            "(150, 2)\n",
            "(150, 1)\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF-wtxZlM9BX",
        "colab_type": "code",
        "outputId": "2a9747bf-8824-4a0b-b942-d39ca5abe361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def test_generateXY():\n",
        "    X_train, Y_train = generateXY(50)\n",
        "    assert X_train.shape==(150,2)\n",
        "    assert Y_train.shape==(150,1)\n",
        "    print(\"OK\")\n",
        "test_generateXY()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 2) X.shape\n",
            "(150,)\n",
            "(150, 2)\n",
            "(150, 1)\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtdHtV8oM9Bk",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "Now our Y is in the form [0], [1] and [2]. We want to convert them to [1,0,0], [0,1,0] and [0,0,1], respectively. \n",
        "Could you write a code to convert Y (with one column) into one-hot encoded Y (with 3 columns)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGsCK-1yM9Bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def oneHot(y, Ny):\n",
        "    '''\n",
        "    Input:\n",
        "        y: an int in {0, 1, 2}\n",
        "        Ny: Number of classes, e.g., 3 here.\n",
        "    Output:\n",
        "        Y: a vector of Ny (=3) tuples\n",
        "    '''\n",
        "    Y = np.zeros(Ny)\n",
        "    Y[y] = 1\n",
        "    return Y\n",
        "\n",
        "Ny = 3\n",
        "Y_train = np.array([oneHot(y,Ny) for y in Y_train_int])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbps14rJM9Br",
        "colab_type": "code",
        "outputId": "723647f1-ee4f-4d0a-c18d-6f1fceb8561c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_oneHot():\n",
        "    assert np.all(oneHot(0,3)==np.array([1,0,0]))\n",
        "    assert np.all(oneHot(1,3)==np.array([0,1,0]))\n",
        "    assert np.all(oneHot(2,3)==np.array([0,0,1]))\n",
        "    assert Y_train.shape[1]==3\n",
        "    print(\"OK\")\n",
        "test_oneHot()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_-e2roJM9B0",
        "colab_type": "text"
      },
      "source": [
        "### Input Normalization\n",
        "X can lie in any unbounded range. We need to curtail to a narrow range close to zero. This helps in enhancing the stability of training and hyper-parameter tuning.\n",
        "This is normally achieved by scaling the X to have zero mean and unit standard deviation (std).\n",
        "\n",
        "$X \\leftarrow \\frac{X-mean(X)}{std(X)}$, where this is element wise division\n",
        "\n",
        "Could you use training samples to find mean and std, and normalize your X_train with that?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByV7uf03M9B3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def findMeanStddev(X):\n",
        "    '''\n",
        "    Input: \n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "    Output:\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X)\n",
        "    '''\n",
        "    mean = np.sum(X, axis=0)/X.shape[0]\n",
        "    X1 = X-mean\n",
        "    stddev = np.sqrt(np.sum(X1*X1, axis=0)/X.shape[0])\n",
        "    return mean, stddev\n",
        "\n",
        "def normalizeX(X, mean, stddev):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "        mean: mean of samples in X (same size as X)\n",
        "        stddev: element-wise std dev of sample in X (same size as X) \n",
        "    Output:\n",
        "        Xn: X modified to have 0 mean and 1 std dev\n",
        "    '''\n",
        "    Xn = (X-mean)/(stddev+1e-8)\n",
        "    return Xn\n",
        "\n",
        "mean_train, stddev_train = findMeanStddev(X_train)\n",
        "X_train = normalizeX(X_train, mean_train, stddev_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyuQ0HnqM9CB",
        "colab_type": "code",
        "outputId": "44f3d9ea-48f1-4916-8f15-3920fbada309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_normalizeX():\n",
        "    X = np.ones((3,3))\n",
        "    m,s = findMeanStddev(X)\n",
        "    assert np.all(m==np.ones(3))\n",
        "    assert np.all(s==np.zeros(3))\n",
        "    assert np.all(normalizeX(X,m,s)==0*X)\n",
        "    assert mean_train.shape==(2,)\n",
        "    assert stddev_train.shape==(2,)\n",
        "    # test on random X\n",
        "    X = np.random.random((5,3))\n",
        "    m,s = findMeanStddev(X)\n",
        "    Xn = normalizeX(X,m,s)\n",
        "    mn, sn = findMeanStddev(Xn)\n",
        "    assert np.allclose(mn, np.zeros(3))\n",
        "    assert np.allclose(sn, np.ones(3))\n",
        "    print(\"OK\")\n",
        "test_normalizeX()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idfaqf4OM9CM",
        "colab_type": "text"
      },
      "source": [
        "### Plotting\n",
        "Could you plot all the samples in X_train with different colors for different classes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMyaxtlJM9CQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "089f8acd-5f93-4a4d-c484-c62523d3af72"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "def plotXY(X, Y):\n",
        "    '''\n",
        "    Inputs:\n",
        "        X: a matrix of size (no. of samples, dimension of each sample)\n",
        "        Y: a matrix of size (no. of samples, no. of classes) - these are one-hot vectors\n",
        "    Action:\n",
        "        Plots the samples in X, their color depends on Y\n",
        "    '''\n",
        "    Ny = Y.shape[1]\n",
        "    for cls in range(Ny):\n",
        "        idx = np.where(Y[:,cls]==1)[0]\n",
        "        plt.plot(X[idx,0], X[idx,1], colors[cls]+'.')\n",
        "    plt.show()\n",
        "plotXY(X_train, Y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgCZ+X8NxbCUUqNjYQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hb0zbGJM9Cc",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Network\n",
        "We now create the network with dense layers: \n",
        "$y = f(Wx)$\n",
        "\n",
        "ReLU activation: \n",
        "$f(h) = h, h>0; 0, h\\le 0$\n",
        "\n",
        "Softmax activation: \n",
        "$f(h_i) = \\frac{\\exp(h_i)}{\\sum_j \\exp(h_j)}$\n",
        "\n",
        "Categorical cross-entropy loss:\n",
        "$\\mathcal{L} = -\\sum_t y^d_t \\log y_t$\n",
        "\n",
        "Stochastic Gradient Descent:\n",
        "$w_{ij} \\leftarrow w_{ij} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_bXT5x8M9Cd",
        "colab_type": "code",
        "outputId": "ca56293c-e37d-4fb8-9498-9709ab72a60c",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "\n",
        "x = Input(shape=(Nx,))\n",
        "y = Dense(20, activation='relu')(x)\n",
        "y = Dense(Ny, activation='softmax')(y)\n",
        "model = Model(inputs=x, outputs=y)\n",
        "model.compile(optimizer=optimizers.sgd(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/vipular/anaconda3/envs/vipulenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                60        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 63        \n",
            "=================================================================\n",
            "Total params: 123\n",
            "Trainable params: 123\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAyqM6U0M9Cm",
        "colab_type": "text"
      },
      "source": [
        "### Plotting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpiTEmxJM9Co",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d3d0c3d-a06f-43b7-b964-ca4a53b2479b"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "def plotModel(model):\n",
        "    plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "    from IPython.display import Image\n",
        "    Image(retina=True, filename='model.png')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNmbpH6tM9Cu",
        "colab_type": "text"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3NyeI8HM9Cw",
        "colab_type": "code",
        "outputId": "4fbacfca-6140-411a-f833-81ed2e62540d",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=50) # validation_split = 0.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/vipular/anaconda3/envs/vipulenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.5571 - acc: 0.8267\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 144us/step - loss: 0.5452 - acc: 0.8267\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 151us/step - loss: 0.5325 - acc: 0.8267\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 136us/step - loss: 0.5213 - acc: 0.8267\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 149us/step - loss: 0.5104 - acc: 0.8267\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 149us/step - loss: 0.4992 - acc: 0.8267\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 176us/step - loss: 0.4894 - acc: 0.8267\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 162us/step - loss: 0.4806 - acc: 0.8267\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 169us/step - loss: 0.4715 - acc: 0.8267\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 370us/step - loss: 0.4630 - acc: 0.8267\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 158us/step - loss: 0.4556 - acc: 0.8333\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 183us/step - loss: 0.4481 - acc: 0.8333\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 156us/step - loss: 0.4406 - acc: 0.8333\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 169us/step - loss: 0.4339 - acc: 0.8333\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 148us/step - loss: 0.4273 - acc: 0.8333\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 177us/step - loss: 0.4206 - acc: 0.8333\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 195us/step - loss: 0.4147 - acc: 0.8467\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 176us/step - loss: 0.4097 - acc: 0.8467\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 267us/step - loss: 0.4040 - acc: 0.8467\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 151us/step - loss: 0.3980 - acc: 0.8467\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 142us/step - loss: 0.3927 - acc: 0.8467\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 174us/step - loss: 0.3882 - acc: 0.8467\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 196us/step - loss: 0.3835 - acc: 0.8467\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 184us/step - loss: 0.3793 - acc: 0.8467\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 144us/step - loss: 0.3751 - acc: 0.8400\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 154us/step - loss: 0.3715 - acc: 0.8467\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 170us/step - loss: 0.3676 - acc: 0.8467\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 172us/step - loss: 0.3644 - acc: 0.8467\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 154us/step - loss: 0.3603 - acc: 0.8533\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 176us/step - loss: 0.3570 - acc: 0.8533\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 140us/step - loss: 0.3538 - acc: 0.8667\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 186us/step - loss: 0.3508 - acc: 0.8667\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 193us/step - loss: 0.3477 - acc: 0.8667\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 161us/step - loss: 0.3446 - acc: 0.8667\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 144us/step - loss: 0.3420 - acc: 0.8733\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 166us/step - loss: 0.3395 - acc: 0.8667\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 136us/step - loss: 0.3371 - acc: 0.8600\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 148us/step - loss: 0.3350 - acc: 0.8600\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 136us/step - loss: 0.3326 - acc: 0.8600\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 170us/step - loss: 0.3303 - acc: 0.8600\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 188us/step - loss: 0.3286 - acc: 0.8600\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 187us/step - loss: 0.3266 - acc: 0.8600\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 148us/step - loss: 0.3245 - acc: 0.8600\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 162us/step - loss: 0.3228 - acc: 0.8600\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 174us/step - loss: 0.3212 - acc: 0.8600\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 119us/step - loss: 0.3192 - acc: 0.8600\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 150us/step - loss: 0.3179 - acc: 0.8600\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 161us/step - loss: 0.3160 - acc: 0.8600\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 144us/step - loss: 0.3146 - acc: 0.8600\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 129us/step - loss: 0.3134 - acc: 0.8600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSWKLcsJM9C4",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation\n",
        "Could you:\n",
        "- Generate 20 samples from each class\n",
        "- Normalize them with mean_train and std_train\n",
        "- Get Y_test as one hot encoded labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3QB669hM9C7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test, Y_test_int = generateXY(20)\n",
        "X_test = normalizeX(X_test, mean_train, stddev_train)\n",
        "Y_test = np.array([oneHot(y,Ny) for y in Y_test_int])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "738j_68KM9DA",
        "colab_type": "code",
        "outputId": "4f429589-bccb-43a4-f450-5526a54f9bf7",
        "colab": {}
      },
      "source": [
        "def test_testData():\n",
        "    assert Y_test.shape==(60,3)\n",
        "    assert X_test.shape==(60,2)\n",
        "#     mn, sn = findMeanStddev(X_test)\n",
        "#     assert np.allclose(mn, np.zeros(2), atol=1)\n",
        "#     assert np.allclose(sn, np.ones(2), atol=1)\n",
        "    print(\"OK\")\n",
        "test_testData()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCMtdwWHM9DI",
        "colab_type": "code",
        "outputId": "4b113d85-93de-4ab1-fbf8-933d8292c109",
        "colab": {}
      },
      "source": [
        "## Evaluation\n",
        "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)  # Evaluate the model\n",
        "print('Accuracy :%0.3f'%accuracy)\n",
        "Y_pred = model.predict(X_test)\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print( confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1)) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :0.867\n",
            "[[13  3  4]\n",
            " [ 1 19  0]\n",
            " [ 0  0 20]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}